\documentclass[
 reprint,
 superscriptaddress,
 amsmath,amssymb,
 aps,
]{revtex4-1}

\usepackage{graphicx}
\usepackage{dcolumn}
\usepackage{bm}
\usepackage[separate-uncertainty=true,scientific-notation=true]{siunitx}
\usepackage{mhchem}
\usepackage{tikz}
\usepackage{tikzscale}

\newcommand{\rmd}{\rm{d}}
\newcommand{\diff}[2]{\frac{\rmd#1}{\rmd#2}}
\newcommand{\acf}{\rm{ACF}}
\newcommand{\angstrom}{\rm{\AA}}
\newcommand{\scinot}[2]{ {#1} \times 10^{#2} }

\begin{document}

\title{A tutorial on the model-dependent analysis of neutron and X-ray reflectometry}% Force line breaks with \\
\thanks{This work was the fortuitous result of the COVID-cancellation of the ISIS Neutron Training Course in March 2020.}
    
\author{Andrew R. McCluskey}
  \email{andrew.mccluskey@ess.eu}
  \email{a.r.mccluskey@bath.ac.uk}
  \affiliation{Data Management and Software Centre, European Spallation Source ERIC, Ole Maal{\o}es vej 3, 2200 Copenhagen, DK}
  \affiliation{Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, UK}
\author{Tom Arnold}
  \affiliation{European Spallation Source ERIC, P.O. Box 176, SE-221 00 Lund, Sweden}
  \affiliation{Diamond Light Source, Rutherford Appleton Laboratory, Harwell Science and Innovation Campus, Didcot, OX11 0DE, UK}
  \affiliation{ISIS Pulsed Neutron and Muon Source, Science and Technology Facilities Council, Rutherford Appleton Laboratory, Harwell Science and Innovation Campus, Didcot, Oxfordshire, OX11 0QX, UK}
  \affiliation{Department of Chemistry, University of Bath, Claverton Down, Bath BA2 7AY, UK}
\author{Juan M. Carmona Loaiza}
  \affiliation{J\"ulich Centre for Neutron Science (JCNS) at Heinz Maier-Leibnitz Zentrum (MLZ).
	Lichtenbergstra√üe 1, 85748 Garching, Germany.}
\author{Joshaniel F. K. Cooper}
  \affiliation{ISIS Pulsed Neutron and Muon Source, Science and Technology Facilities Council, Rutherford Appleton Laboratory, Harwell Science and Innovation Campus, Didcot, Oxfordshire, OX11 0QX, UK}
\author{Nicola K. Farmer}
  \affiliation{Diamond Light Source, Rutherford Appleton Laboratory, Harwell Science and Innovation Campus, Didcot, OX11 0DE, UK}
  \affiliation{Department of Physics, University of York, Heslington, York, YO10 5DD, UK}
\author{James H. Durant}
  \affiliation{ISIS Pulsed Neutron and Muon Source, Science and Technology Facilities Council, Rutherford Appleton Laboratory, Harwell Science and Innovation Campus, Didcot, Oxfordshire, OX11 0QX, UK}
  \affiliation{Department of Computer Science, University of Warwick, Coventry, CV4 7AL, UK}
\author{Andrew R. J. Nelson}
  \affiliation{Australian Centre for Neutron Scattering, Australian Nuclear Science and Technology Organisation, Locked Bag 2001, Kirrawee DC, NSW 2232, Australia}
\author{Wojciech Potrzebowski}
  \affiliation{Data Management and Software Centre, European Spallation Source ERIC, Ole Maal{\o}es vej 3, 2200 Copenhagen, DK}
\author{Mark L. Schlossman}
  \affiliation{Department of Physics, University of Illinois at Chicago, Illinois 60607, USA}
\author{Tim Snow}
  \affiliation{Diamond Light Source, Rutherford Appleton Laboratory, Harwell Science and Innovation Campus, Didcot, OX11 0DE, UK}
  \affiliation{School of Chemistry, University of Bristol, Bristol, BS8 1TS, UK}


\collaboration{Open Reflectometry Standards Organisation}%\noaffiliation


\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
    Neutron and X-ray reflectometry are powerful experimental techniques for probing the structure of surfaces and buried interfaces. 
    However, the analysis of neutron and X-ray analysis is an inherently ill-posed problem; the well-known phase problem leading to different structures whose scattering signals are very similar. 
    This leads to the use of model-dependent anlaysis processes to interpret the experimental data, where information that is ``known'' about the system is integrated into the analysis. 
    This tutorial aims to introduce model-dependent analysis method for neutron and X-ray reflectometry methods, focusing on areas of similarity between the two techniques and signposting where they differ with relevant further reading, and discuss in detail its important mathematical aspects.
    We hope that those that are well experienced in reflectometry methods will find this resource as useful as those who are about to perform these first experiment. 
\end{abstract}

\maketitle

\section{Introduction}

% What do NR and XRR measure?

Neutron and X-ray reflectometry are experimental techniques where the probing radiation (the neutron or X-rays) are incident at a shallow angle on an interface. 
The radiation is elastically scattered normal to the surface with a scattering angle equal to the incidence angle, and detected using some detector (Figure~\ref{fig:beamline}), which is radiation specific \cite{pietropaolo_neutron_2020,looker_synchrotron_2020}.
Specular reflectometry measurements are where the incident angle ($\theta_i$) and scattered angle ($\theta_r$) are the same and the scattered angle is only non-zero in the $z$-dimension. 
The condition where $\theta_i \neq \theta_r$ and there is no scattering in the $x/y$-plane represents off-specular scattering \cite{richardson_study_1997,dai_comparative_2011}, while where scattering is present in the $x/y$-plane is commonly known as grazing incidence scattering, grazing incidence small angle scattering or grazing incidence wide angle scattering \cite{renaud_probing_2009,smilgies_scherrer_2009,hexemer_advanced_2015}. 
These measurements can be complementary to specular reflectometry, but are not the focus of this work. 
%
\begin{figure}[t]
    \includegraphics[width=0.4\textwidth]{beamline}
    \caption{A pictorial representation of a reflectometry beamline with a vertical scattering geometry; where the scattering angle, $\theta$ is shown and the prodbing radiation beam is given as a thick solid black line. Note, that the radiation is only scattered in the $z$-dimension, such that the $\theta_i=\theta_r$, and not in the $x/y$-plane.}
    \label{fig:beamline}
\end{figure}
%

Typical systems investigated with specular reflectometry methods include, but are not limited to, thin films \cite{delcea_xray_2019} and chemical aggregation \cite{ma_unusual_2021} at interfaces, biological membranes \cite{john_large_2021} and magnetic nanostructures \cite{liu_coherent_2020}.
Reflectometry methods can be used to probe the structure of essentially any nanostructured interface that offers sufficient contrast in the amount that the radiation is scattered. 
This work will introduce and disucss the more mathematically directed elements of reflectometry data analysis; including the calculation of reflectivity from models, the effect of resolution on analysis and the importance of optimisiation methods and uncertainty quantification.
To ensure brevity, it will not focus on the role of polarisation, either in neutron or X-ray, in the analysis of reflectometry data. 
For those interested in polarised neutron reflectometry analysis, we suggest the work of Fitzsimmons, Toperverg and others \cite{fitzsimmons_applications_2005,zabel_polarized_2007,toperberg_neutron_2015}.

This work will assume that the data has been reduced, including aspects such as background subtraction \cite{salah_data_2007} and normalisation. 
The reduced data is described in terms of reflectivity, $R(q)$, which depends on the wavevector transfer normal to the interface (this dimension is commonly known as $z$ within the reflectometry community), written as $q_z$ or $q$, and is measured as, 
%
\begin{equation}
    R(q) = \frac{\text{specular reflected intensity at }q}{\text{incident radiation intensity}},
    \label{equ:refl}
\end{equation}
%
where, the denominator is the total flux of neutrons or X-rays incident on the sample. 
The wavevector transfer, $q$, depends on the scattering angle, $\theta$, and wavelength, $\lambda$ of the incident radiation, 
%
\begin{equation}
    q = \frac{4\pi\sin\theta}{\lambda}.
    \label{equ:qz}
\end{equation}
%
The full derivation of this elastic scattering wavevector transfer can be found in the work of Sivia \cite{sivia_elementary_2011}. 
The $R(q)$ in Equation~\ref{equ:refl} is the value that we can obtain from our structural model and compare with the data, and then through some iterative process improve the model. 

The Born approximation \cite{born_quantenmechanik_1926}, which assumes a single scattering event for each incident raditation particle, allows the reflectivity to be calculated as follows \cite{sivia_elementary_2011},
%
\begin{equation}
    R(q) \approx \frac{16\pi^2}{q^4} \bigg| \int^{+\infty}_{-\infty}{\langle\rho'(z)\rangle_{x,y}\exp{(-\mathrm{i} zq) \text{d}z} \bigg|^2},
    \label{equ:kine}
\end{equation}
%
where $\langle\rho'(z)\rangle_{x,y}$ is the first derivative of the scattering length density profile averaged over the x-y plane of the interface illuminated by the beam.
The scattering length density profile is our model, a functional description of how the scattering length varies with depth normal to the interface, $z$. 
Note that reflectometry measurements only contain information about the scattering length density profile as a function of $z$.
The scattering length density of a material is calculated by summing over the scattering lengths of all of the different atoms in a material and dividing by the molecular volume (which itself can be found from the mass density of a material), $V_m$,
%
\begin{equation}
    \rho_m = \frac{\sum_i{b_i}}{V_m}.
\end{equation}
%
For interactions with neutrons the scattering length of an atom is the coherent scattering length \cite{sears_neutron_1992,dianoux_neutron_2003}, while for X-rays the scattering length of an atom is energy dependent \cite{seltzer_xray_1995}, but to a first approximation can be calculated as $b_i = Z_ir_e$, where $Z_i$ is the atomic number and $r_e$ is the classical electron radius of $\SI{2.871e-5}{\angstrom}$. 

The approach based on the Born Approximation outline above is referred to as the kinematic approach, and it has a significant problem that can be demonstrated by consider the simple case of a bare silicon surface \cite{sivia_elementary_2011}, which we will model with a Heaviside function, shown in Figure~\ref{fig:kine}(a), 
%
\begin{equation}
    \rho(z) =
    \begin{cases}
        0 & \text{where } z < 0,\\
        \rho_{\text{Si}} & \text{otherwise},
    \end{cases}
\end{equation}
%
%
\begin{figure}[t]
    \includegraphics[width=0.45\textwidth]{kine}
    \caption{A graphical representation of each aspect of the kinematic approach; (a) the Heaviside function describing the scattering length density profile for a bare silicon substrate, (b) the $\delta$-function arising from the first derivative of the function in (a), and (c) the resulting reflectometry profile, where the orange line at $R(q)$ = 1 identifies the breakdown between experiment and theory.}
    \label{fig:kine}
\end{figure}
%
where, the scattering length density is that of silicon for neutrons, $\SI{2.074e-6}{\angstrom^{-2}}$. 
The first derivative of a Heaviside function is a scaled $\delta$-function, shown in Figure~\ref{fig:kine}(b), 
%
\begin{equation}
    \rho'(z) = \rho_{\text{Si}}\delta(z).
\end{equation}
%
Then, using Equation~\ref{equ:kine}, the reflectivity can be calculated with respect to $q$, 
%
\begin{equation}
    \begin{aligned}
    R(q) & \approx \frac{16\pi^2}{q^4} \bigg| \rho_{\text{Si}}\int^{+\infty}_{-\infty}{\delta(z)\exp{(-\mathrm{i} zq) \text{d}z}} \bigg|^2 \\
     & \approx \frac{16\pi^2}{q^4} \bigg| \rho_{\text{Si}} \exp{(0)} \bigg| ^2 \\
     & \approx \frac{16\pi^2\rho_{\text{Si}}^2}{q^4}.
    \end{aligned}
    \label{equ:baresi}
\end{equation}
%
The result of Equation~\ref{equ:baresi} is shown in Figure~\ref{fig:kine}(c), where it is clear that the agreement with an experimental profile would be poor as $q \to 0$ \cite{majkrzak_exact_1998} at which the reflectivity would be greater than \num{1}. 
This violates the physical condition imposed by Equation~\ref{equ:refl}, with more of the probing radiation being reflected than were incident in the first place. 
The Born Approximation is reasonably accurate when $q > 4q_c$, where $q_c$ is the critical wavevector for total reflection. 
Although it is not often used to fit data, the utility of the Born Approximation is found in the analytical simplicity, providing insight into the form of the reflectivity curve for a given model \cite{pershan_liquid_2012}.

\section{Dynamical approach}
\label{sec:dyna}
It is possible to obtain higher accuracy through the use of the Abel\`{e}s matrix formalism \cite{abeles_sur_1948} or Parratt \cite{parratt_surface_1954} recursive model for the reflection of light at a given number of stratified interfaces, also referred to as the dynamical approach. 
This alternative method for calculating the reflected intensity from a model, where the model is now a series of layers each with a given scattering length density. 
The probing radiation is then reflected and transmitted at the interface between each of the layers.
Figure~\ref{fig:refr} shows the paths of the waves through a simple two layer system, where layer \num{0} is a vacuum above the sample. 
It is clear, by analogy to Bragg's law, how the two waves, labelled $r$, could interfere constructively or destructively depending on the thickness of the layer \num{1}, $d$. 
%
\begin{figure}[t]
    \includegraphics[width=\linewidth]{dynamic}
    \caption{A schematic diagram showing the reflected ($r$) and transmitted ($t$) waves when an incident ($i$) wave enters an interface of thickness $d$. The angle between the incident wave and the surface is the scattering angle $\theta$.}
    \label{fig:refr}
\end{figure}
%

The generalisation of this approach to any number of layers is possible and enables the reflected intensity to be calculated at each value of $q$ that was measured. 
The incident radiation is reflected by each of the interfaces, with the wavevector normal to the surface, $k_n$, in each layer being given by, 
%
\begin{equation}
    k_n = \sqrt{k_0^2 - 4\pi(\rho_n - \rho_0)},
\end{equation}
%
where $k_0 = 0.5q$. 
The Fresnel reflectance between layers $n$ and $n+1$, $r_{n, n+1}$, can then be found along with the phase factor, $\beta_n$, for the layer $n$, 
%
\begin{equation}
    r_{n, n+1} = \frac{k_n - k_{n+1}}{k_n + k_{n+1}},
    \label{equ:fres}
\end{equation}
%
%
\begin{equation}
    \begin{aligned}
        \beta_n &= ik_nd_n, \\
        \beta_0 &= 0. \\
    \end{aligned}
\end{equation}
%
From the Fresnel reflectance and phase factor, a characteristic matrix for the interface between two layers, $M_{n, n+1}$, can be constructed, 
%
\begin{equation}
    M_{n, n+1} =
    \begin{bmatrix}
        \exp{\beta_n} & r_{n,n+1}\exp{-\beta_n} \\
        r_{n,n+1}\exp{\beta_n} & \exp{\beta_n}.
    \end{bmatrix}
\end{equation}
%
The resultant matrix for a given $q$-vector, $B(q)$, is found from the product of all of the characteristic matrices, 
%
\begin{equation}
    B(q) = \prod_{n=0}^{n_{\text{max}}-1}{M_{n, n+1}}.
\end{equation}
%
The characteristic matrix for each interface describes its reflectance and transmittance, with the product describing the reflectance and transmittance propagated through all the interfaces. 
This can then be used to calculate the overall reflectance and reflectivity of the system,
%
\begin{equation}
    \begin{aligned}
        r &=  \frac{B_{0,1}(q)}{B_{0,0}(q)}, \\
        R(q) &= r\bar r.
    \end{aligned}
\end{equation}
%

Applying this dynamical approach to a flat silicon surface is shown in Figure~\ref{fig:dyna}.
There is a clear difference between the kinematic and dynamical approaches as $q \to 0$, with the dynamical approach adhering to the physical constraint that is not found in the kinematic approach. 
Figure~\ref{fig:dyna} also illustrates the accuracy of the Born approximation for $q > 4 q_c$, where $4q_c\approx\SI{0.04}{\angstrom^{-1}}$.%WP: Would it be useful to show exact calculations for silicon substrate (probably in supplementary materials)?
%
\begin{figure}[t]
    \includegraphics[width=0.49\textwidth]{dyna}
    \caption{A comparison of the kinematic (blue) and dynamical (green) approaches to determine the reflected intensity from the material with the scattering length density profile given in Figure~\ref{fig:kine}(a).}
    \label{fig:dyna}
\end{figure}
%

The dynamical method defines the layers as perfectly sharp interfaces, which may not be strictly true. 
To account for potential roughness or interdiffusion, correction terms are added to Equation~\ref{equ:fres}. 
The most common of these is N\'{e}vot Croce Gaussian broadening \cite{nevot_caracterisation_1980}, in which the Fresnel reflectance is evaluated as, 
%
\begin{equation}
    r_{n, n+1} = \frac{k_n - k_{n+1}}{k_n + k_{n+1}} \exp{(-2k_nk_{n+1}\sigma^2_{n,n+1})},
\end{equation}
%
where $\sigma_{n, n+1}$ is the interfacial roughness between the layers $n$ and $n+1$.

Figure~\ref{fig:rough} shows the effect that increasing roughness can have on the scattering length density of some system, in this case a \SI{10}{\angstrom} layer of nickel on titanium. 
Notice, that as the roughness increases to values similar to half of the layer thickness, artifacts begin to appear in the scattering length density profile \cite{tolan_xray_1999}. 
If the roughness is too large, it begins to effect the scattering length density on the next layer in the material. 
Furthermore, as the roughness correction for the Fresnel reflectance only influences the indiviudal interfaces, this unphysical roughness is not represented in the model reflectometry. 
%
\begin{figure}[t]
    \includegraphics[width=0.45\textwidth]{roughness}
    \caption{The effect of a large roughnes on the scattering length density profile. The material modelled is the X-ray scattering length denisty profile for a \SI{10}{\angstrom} thick layer of Ni on a Ti substrate at the interface with vacuum. The roughness between the vacuum and the Ni is varied from \SI{3}{\angstrom} (blue), through \SI{5}{\angstrom} (orange), to \SI{7}{\angstrom} (green). The full scattering length density profile is given in (a), while (b) represents a zoomed in representation.}
    \label{fig:rough}
\end{figure}
%

% Microslicing 
% AJC Should we give the slicing example as a way to overcome this issue? Also I think we should make it explicit that not only is the SLD unphysical but it does not match the simulated reflectivty curve.
% Plot a microsliced reflectometry against that for the none sliced

\section{The effect of resolution on analysis}

% How is this used in analysis

\section{Optimisation}

% Local and global optimisation

% Briefly metion multi-model optimsation?

\section{Uncertainty quantification}

% Gradient descent

% Sampling methods

\section{Overfitting}

\section{Ill-posedness}

% Autocorrelation function

\section{Conclusions}


\section{Instrumental resolution}
In the analysis process, a resolution function associated with the instrumentation that the reflectometry measurement was performed on is typically introduced to the model.
This ensures greater accuracy in the modelled reflectometry to the experiment.
These resolution functions are non-trival to define, however, the majority of reflectometry instruments will be able to offer information about the appropriate function to use.

Here, we will briefly discuss the introduction of a simple Gaussian resolution function with a constant $\frac{\text{d}q}{q}$.
The Gaussian function is centred on a given wavevector, $q$, and has a width at half maximum of $\text{d}q$.
For each $q$-value in the unsmeared data, a Gaussian function is produced which is convolved with the unsmeared data to produce data that includes some description of the instrumental resolution.
Typically, this convolvutional smearing is included in the reading of the experimental data.
However, the resolution function used by a given software package may not be an accurate description of that from the instrument, therefore we recommend discussing this with the members of instrument staff.
More information about the resolution function can be found in the work of van Well and Fredrikze \cite{vanwell_resolution_2005} and Nelson and Dewhurst \cite{nelson_towards_2013,nelson_towards_2014}

\section{Ill posedness - the phase problem}
%WP: I suggest moving this section towards end.
Figure \ref{fig:phase_problem_sld} shows two hypothetical samples of the same total thickness, consisting of one and two layers, deposited on a substrate with $\rho_s = \SI{2.07e-6}{\angstrom^{-2}}$.
Even though the scattering length density depth profiles are distinctly different, the resultant reflectivity curves are virtually identical, with only a slight discrepancy at the first minimum.
This ambiguity, known as \emph{the phase problem}, is an example of the information lost in diffraction experiments due to the inability of measuring the phase of the scattered wave (note that equation \ref{equ:kine} involves a modulus-squared).
\begin{figure}
    \includegraphics[width=0.49\textwidth]{phase_problem_SLD.pdf}
    \caption{The phase problem, (a) two different samples deposited on the same substrate with a scattering length density of $\rho_s$, (b) are indistinguishable from one another by looking only at their reflectivity curve $R(q)$.}
    \label{fig:phase_problem_sld}
\end{figure}
%WP: I think some gentle introduction to autocorelation function will be useful
A heuristic explanation for the phase problem can be built in terms of the auto-correlation function of $\rmd\rho/\rmd z$, which gives a real-space representation of the information contained in the reflectivity pattern,
%
\begin{equation}
    \begin{aligned}
        \acf_{\rho'}(z) & = \int_{-\infty}^{\infty} \rho'(z)^* \, \rho'(z + \xi) \, \rmd \xi \\
                        & = \int_{-\infty}^{\infty} \left| \widehat{\rho'}(q)  \right|^{2} \rm{e}^{iqz} \rmd q,
    \end{aligned}
    \label{equ:acf}
\end{equation}
%
where the hat notation indicates a Fourier transform, the star notation a complex conjugate, and the prime notation a derivative.
\begin{figure}
    \includegraphics[width=0.49\textwidth]{phase_problem_ACF.pdf}
    \caption{The phase problem explanation: (a) the derivatives of the two hypothetical scattering length density profiles from Figure~\ref{fig:phase_problem_sld} are scaled $\delta$ functions peaking at the interfaces between scattering length density layers, (b) the autocorrelation functions, $\acf_{\rho'}(z)$ are almost identical between the samples, peaking at exactly the same locations.}
    \label{fig:phase_problem_acf}
\end{figure}

As $\rho'(z)$ is purely real and in both examples is only composed of $\delta$ functions, the ACF can be obtained by direct inspection (See Figure~\ref{fig:phase_problem_acf} and Equation~\ref{equ:acf}):
\begin{itemize}
    \item At zero lag, there is always full correlation, thus the ACF will have a large positive peak at the origin for both samples.
    \item For both samples, the interference between the front and back interfaces, at $z = 0$ and $z = \SI{200}{\angstrom}$ respectively, translates into a pair of $\delta$ peaks with opposite signs in $\rho'$. Thus, at a $\xi = \pm \SI{200}{\angstrom}$ lag, the ACF must peak with negative amplitude.
    \item For the two-layer sample, an extra pair of peaks in the ACF could be expected at lags of $\xi = \pm \SI{100}{\angstrom}$; however, since there is a complete cancellation between the positive and the negative contributions of the $\delta$ functions at $z = 0$ and $z = \SI{200}{\angstrom}$, the expected extra pair of peaks is absent.
\end{itemize}
As the scattering length density profiles of both samples translate into the same ACF, it is now not surprising the similarity between their reflectivity curves.

A deeper discussion on the phase problem and some techniques that have been developed to tackle it are described in the work by Majkrzak \emph{et al.} \cite{majkrzak_phase_2003}. Here, they give an overview of several phase-sensitive experimental methods and claim that any ambiguity in a scattering length density profile obtained by these is a consequence only of the limited $q$ range over which reflectivities can be measured.
In a similar fashion, Koutsioubas \cite{koutsioubas_model_2019} proposes a model-independent method which, when applied to multiple solvent contrast data, should lead to reliable reconstructions of the interfacial structure without the need for any a priori assumptions.

However, even when the phase problem can be theoretically alleviated, reflectivity curves from different scattering length density profiles may still be close enough to one another to be indistinguishable by finite-resolution experiments, effectively keeping the analysis of reflectometry profiles in the realm of ill-posed non-invertible problems.
As such, reflectometry data analysis demands for advanced trial-and-error fitting techniques, the daily bread of experimenters to which the following section is devoted.

\section{Global optimisation}

The recursive method described in Section~\ref{sec:dyna} gives an accurate method to obtain a model of reflected intensity.
However, this is just the first step in the analysis of a neutron reflectometry dataset.
Now, we are interested in optimising our model such that the reflected intensity from it matches our experimental data as best as possible.
This is the problem of parameter optimisation, which is a broad area of mathematics and computer science that we will not dwell on here.
%AJC Should a reference of a review be included?
However, we will introduce the basics of optimisation and discuss the most common global optimisation method used in reflectometry.

When we measure a reflectometry profile, we measure the reflectivity, and some uncertainty in that measurement, at discrete points in the wavevector, $R(q) \pm \delta R(q)$.
Using the recursive method discussed above, we can calculate a model reflected intensity at these same $q$ values, $R_m(q)$.
We then aim to reduce the difference between the measured and modelled reflected intensity through the optimisation (maximisation) of the likelihood, $L$, % AJC Maybe lead in with the use of Chi^2 and how the Likelihood function relates. Also that usually the negative log likelihood function is used for easier problem optimisation for computation
%
\begin{equation}
    \begin{aligned}
        L = \exp\bigg\{ & - \frac{1}{2} \sum_{q=q_{\text{min}}}^{q_{\text{max}}} \bigg[\frac{R(q) - R_m(q)}{\delta R(q)}\bigg]^2 \\
         & + \ln[2\pi \delta R(q)]\bigg\}.
    \end{aligned}
\end{equation}
%
This parameter describes the probability that the model accurately represents the observed reflectometry data.
The aim in reflectometry modelling is to obtain a model reflectivity that has the maximum likelihood possible for the given experimental data.
Figure~\ref{fig:likelihood} shows the maximum likelihood model for some experimental data, in addition to another model which does not agree well with the data and therefore has a lower likelihood as a result.
%
\begin{figure}[t]
    \includegraphics[width=0.49\textwidth]{likelihood}
    \caption{The maximum likelihood fit, the blue data points are experimental data, while the yellow line shows the maximum likelihood model fit to this dataset and the green line shows another model (which doesn't maximise the likelihood).}
    \label{fig:likelihood}
\end{figure}
%

The global optimisation of a reflectometry model is a particularly difficult problem, since there may be multiple solutions to a particular reflectometry profile.
However, a particular global optimisation method has shown substantial utility in the fitting of neutron reflectometry data \cite{varderlee_comparison_2007}: differential evolution \cite{StornPrice_1997_DE,wormington_characterization_1999}.
This has lead to the inclusion of this method in many common reflectometry analysis packages \cite{bjorck_fitting_2011,nelson_refnx_2019,kienzle_ncnr_nodate}.

Differential evolution is an iterative, genetic algorithm, designed to mimic the evolution processes observed in biology \cite{holland_adaptation_1992}.
The method consists of two vectors, the parent population $\mathbf{p}$, and the offspring population, $\mathbf{o}$.
These vectors are of shape $(i \times j)$, where $i$ is the number of parameters in the model and $j$ is the number of candidate solutions being considered.
The offspring population is generated as a result of some trial method, here we will discuss only a simple classical trial method.

A classical trial method consists of two stages, mutation and recombination.
The mutation stage involves the creation of a mutant population, $\mathbf{m}$.
The magnitude of the mutation is dependent on the first of our hyperparameters, the mutation constant, $k_m$,
%
\begin{equation}
    \mathbf{m}_{i,j} = b_i + k_m (\mathbf{p}_{i, R1} - \mathbf{p}_{i, R2}),
\end{equation}
%
where $b$ is the candidate solution with the greatest likelihood, and $\mathbf{p}_{i, R1}$ and $\mathbf{p}_{i, R2}$ are randomly chosen members of the parent population.
The mutation constant hyperparameter controls the size of the search space, with a large $k_m$ corresponding to a wider search.

The recombination step creates the offspring population vector by taking a sample from either the parent or mutant population with some frequency, which depends on our second hyperparameter, the recombination constant, $k_r$,
%
\begin{equation}
    \mathbf{o}_{i, j} =
    \begin{cases}
        \mathbf{m}_{i, j} & \text{where } X < k_r,\\
        \mathbf{p}_{i, j} & \text{otherwise},
    \end{cases}
\end{equation}
%
where, $X$ is a random number selected from a uniform distribution between 0 and 1.
The recombination constant hyperparameter controls the mutation frequency in the offspring population.

The final stage is to compare the offspring and parent population vectors, in the selection stage, to create the parent population for the next iteration.
Here, the likelihood is used to compare between subunits from the offspring or parent populations,
%
\begin{equation}
    \mathbf{p}_{*, j} \leftarrow
    \begin{cases}
        \mathbf{o}_{*, j} & \text{where } L_{\mathbf{o}_{*, j}} > L_{\mathbf{p}_{*, j}},\\
        \mathbf{p}_{*, j} & \text{otherwise},
    \end{cases}
\end{equation}
%
where, the $*, j$ subscript notation indicates all objects from the population, $j$.
Following the use of the differential evolution algorithm, typically a more common (gradient-based) approach is used to ensure that the likelihood has been maximised within the space identified by the differential evolution.
%AJC Interesting suggestion, is it that common? I tend to use DREAM from refl1d and regard that as my finishing parameters (obviously ensureing robustness and that the population and initiallisation parameters are robust). This sentence maybe highlights a topic for another paper fitting methodologies? i.e. how do you fit yours? I think we could probably come up with a large variety of equally valid fitting methods which could be a future toolbox for people to use.

The differential evolution algorithm can be seen in action applied to the negative two-dimensional Ackley function \cite{ackley_connectionist_1987}, in Figure~\ref{fig:ackley}.
The Ackley function is a common function used in the assessment of global optimisation functions. This is due to there being a large number of local minima and only a single global minimum to this function. Here, we want to maximise the value, so the negative Ackley function is used.
This function can maximise the value of the negative Ackley function.
While this does not offer a clear example of this algorithm's application to reflectometry analysis, the popularity of this method in the fitting of reflectometry profiles cannot be denied \cite{bjorck_fitting_2011,nelson_refnx_2019}.
%
\begin{figure}[t]
    \includegraphics[width=0.49\textwidth]{ackley}
    \caption{An example of a differential evolution algorithm applied to the negative of an Ackley function. In this implementation $k_m=0.5$ and $k_r=0.5$. Each line represents a different candidate solution. The optimisation was stopped after 100 iterations had run.}
    \label{fig:ackley}
\end{figure}
%

The analysis of a reflectometry model can take advantage of contrast variation, the exact methods of which are covered in other articles \cite{schurtenberger_contrast_2002}.
However, we note here that, the phrase ``global optimisation'' is often used to describe the analysis of multiple contrasts of the same structure.
This is where a single structural model is used for the analysis of multiple measurements, but the scattering length densities are varied based on the particular contrasts under investigation.
This method has been used to recover the phase information in neutron reflectometry analysis \cite{majkrzak_exact_1995,majkrzak_first_2000,majkrzak_phase_2003,koutsioubas_model_2019}.
However, it is also a tool in model-dependent analysis, as it reduces the number of local minima in the parameter space.
Typically, a global (here meaning multiple contrasts) optimisation criteria is used.
For example, the average or summed (log-)likelihood across all of the different measurements and model reflectometries.

\section{Contrast variation}
The neutron reflectometry profile produced from some system depends on the three factors: 
%
\begin{itemize}
    \item the instrument being used to measure the reflectivity data,
    \item the denisty distribution perpendicular to the interface,
    \item the interaction between the radiation and the matter under investigation.
\end{itemize}
%
This final factor is the ``scattering contrast''; a very useful tool in the analysis of neutron reflectometry data. 
The scattering contrast is defined by the scattering lengths of the atoms scattering the neutrons. 
The value of the scattering length of given atom varies unsystematically with respect to the atomic number and significantly between isotopes of the same element. 
Furthermore, it is dependent on the magnetic state of the atom, allowing the magnetic structure of materials to be probed in addition to the nuclear structure.
 




\section{Uncertainty quantification}
Reflectivity measurements offer an average description of the out of plane structure of a material.
This means that it is pragmatic to describe the uncertainties in the values of our model parameters in the same fashion.
In this section, we will introduce two potential methods to determine the uncertainty on a set of model parameters.

The first is computationally straightforward and is substantially more common.
This is where the parameter uncertainties are found from the Hessian matrix of the optimisation space, where the uncertainties are the square-root of the matrix diagonal.
The previous sentence introduced a lot of terminology that may not be familiar, however we shall work through it slowly.
The above optimisation method, differential evolution, is what is referred to as a stochastic method, where steps taken arise from a randomly determined process.
However, another common type of optimisation methods are known as gradient methods, where the gradient of the optimisation space at a given point is calculated and this is used to determine the next position.
Consider maximising the likelihood for a reflectometry model fit, you would like to make steps that increase the likelihood to reach the maximum, so calculating the gradient will be able to tell you in which direction to travel.

The Hessian matrix, $H$ describes the curvature of optimisation criteria, the likelihood, with respect to each of the model parameters, and it is frequently used in the gradient-based optimisation methods.
This means it is an $i\times i$ matrix, where $i$ is again the number of model parameters, $\theta$. For a two parameter model, this would look like,
%
\begin{equation}
    H =
    \left[\begin{matrix}
        \dfrac{\partial^2 L}{\partial \theta_{1}^2} & \dfrac{\partial^2 L}{\partial \theta_1 \theta_2} \\[6pt]
        \dfrac{\partial^2 L}{\partial \theta_2 \theta_1} & \dfrac{\partial^2 L}{\partial \theta_{2}^2}
    \end{matrix}\right].
\end{equation}
%
If the parameter distribution is Gaussian in shape then the second derivative at the Gaussian maximum will give the variance of that distribution.
Therefore, when these gradient approaches are used, the Hessian matrix is calculated and the diagonal of that matrix gives the uncertainty in each of the parameters.
The off-diagonal elements contain information about the covariance between different parameters, however this is rarely leveraged and the covariances are taken to be negligible.
Additionally, we note that often the Hessian is not calculated in an optimisation so the Jacobian matrix (the first derivative analogue of the Hessian) is used to estimate the Hessian matrix.

The determination of parameter uncertainties from the Hessian matrix is dependent on two major assumptions about the parameter space:
\begin{enumerate}
    \item {the parameters are normally distributed with a single maximum,}
    \item {the covariance between the parameters is negligible.}
\end{enumerate}
However, this may not always be the case \cite{mccluskey_bayesian_2019}. Therefore, it is necessary to use methods to completely describe the parameter probability distribution.
One such method for this is Markov-chain Monte Carlo (MCMC), which samples the posterior probability distribution for each of our parameters to obtain an analytical description \cite{sivia_data_2006}.
Typically, MCMC is used on already optimised solutions to a particular problem. In reflectometry analysis it is usually applied after the differential evolution has optimised the structure.
In addition to being able to quantify the inverse uncertainties (the name given to the uncertainties in the model parameters), MCMC also offers a more complete understanding of the correlations between different parameters \cite{gilks_markov_1995}, which is particularly important in the ill-posed reflectometry analysis.

Once an optimised set of model parameters, $\theta$, are obtained which maximise the likelihood, $L$, some random perturbation is applied to produce a new set of model parameters, $\Theta$,
%
\begin{equation}
    \Theta = \theta + aR,
\end{equation}
%
where $R$ is some normally distributed number centered on \num{0} with a standard distribution of \num{1} and $a$ is the step size.
A new $L$ is found for $\Theta$ and the probability that this transition will occur is found,
%
\begin{equation}
    p = \exp{\bigg[\frac{L(\Theta) - L(\theta)}{2}\bigg]}.
\end{equation}
%
This probability is then compared with a uniformly distributed random number from \num{0} to \num{1}, $n$,
%
\begin{equation}
    \theta \leftarrow
    \begin{cases}
        \Theta & \text{where } n < p,\\
        \theta & \text{otherwise}.
    \end{cases}
\end{equation}
%
This process is repeated until some desired number of samples has been obtained.
It should be noted that it is important to allow the Markov chains to have some ``burn-in'' period, which is not included in the final samples.
This allows the MCMC algorithm to settle into the search-space.
%AJC See my TODO points here
Figure~\ref{fig:mcmc} shows the result of a MCMC sampling for a pair of overlapping Gaussian functions, performed using the \texttt{emcee} Python package \cite{foremanmackey_emcee_2012}.
The posterior probability distributions that are available to each of the four parameters are shown along with the data, the optimised fit and a subset of models within the posterior distributions.
All of the samples from the posterior distributions fit within the uncertainty error bars on the data.
This shows the ability for MCMC to sample the range of the distribution that is allowed by the experimental uncertainty.
%
\begin{figure}[t]
    \includegraphics[width=0.49\textwidth]{mcmc}
    \caption{An example of the result of MCMC uncertainty sampling on data synthesized from two overlapping Gaussian functions. $\theta_1$ and $\theta_2$ are the integrals of the Gaussian functions while $\theta_3$ and $\theta_4$ are the positions. The data is shown with blue circles, the optimised model with an orange line, and samples from the posterior distributions with green lines.}
    \label{fig:mcmc}
\end{figure}
%

We note here that MCMC may also be used to facilitate the use of Bayesian inference in the analysis of reflectometry data, where other prior knowledge about the experimental system is able to influence the result of the analysis.
In this tutorial, we won't say any more on this subject, but if you are interested, this is discussed in the work of Nelson and Prescott \cite{nelson_refnx_2019} and McCluskey and co-workers \cite{mccluskey_general_2020}.

\section{Over-fitting}

When defining a model, care must be taken to balance the complexity of the model with the fit to the data.
By using too few parameters we risk that important structural features are not covered by the model and that fit to the data is sub-optimal.
However over-parameterising the model may lead to over-fitting to the data and therefore deriving false conclusions about the measured sample.
The latter issue is particularly problematic for reflectometry data, for which information content/density is typically low.

Different strategies have been used to address over-fitting in reflectometry.
Typically a ``rule of thumb'' of choosing the most simple model that enables a description of the data has been applied.
Even though such method is easy to apply, it requires prior knowledge about the sample, e.g. from different experimental techniques, and assumptions about information content in the data, i.e. do we have enough information to support the model?
The information content can in principle be quantified directly from the data as has been widely demonstrated for small-angle scattering data, which also typically suffers from limited structural information \cite{Larsen_2018} or by validating the model given the data. %What does the previous sentence mean, like the 'or by validating model given the data' part?%
An example of the latter approach uses Bayesian model selection framework to compare models with different numbers of parameters to determine which offers the maximum information without over-fitting \cite{hughes_model_selection_2019, mccluskey_general_2020}.
While this approach provides rigorous framework for finding an ``optimal'' model it is also important to note that it depends on the prior probabilities chosen for each of the parameters.
The problem of finding an optimal model can also be inverted to another task of acquiring ``optimal'' data that can support a given model.
Treece et al. \cite{Treece_2019} developed a method based on Bayesian statistics and information theory that quantifies information gain from reflectometry experiments with the purpose of guiding experimental design.

Regardless of the method used to mitigate over-fitting, we are limited to information provided by the data.
To overcome this limitation one can supplement data with information from other techniques.
This can be done by constraining model parameters with values obtained from other experiments or by simultaneously fitting the model against multiple experimental data sets.
These can be other reflectometry measurements (e.g. from contrast variation) or from different techniques (e.g. combination of X-ray and neutron reflectometry data).
This can however be challenging because various data may contribute differently to the information content and proper weighting of the information needs to be performed.
Bayesian statistics may alleviate these limitations, however proper frameworks need to be established.

\section{Conclusions}
The aim of this document was to give an introduction to some of the mathematical concepts that underpin modern reflectometry model-dependent analysis.
We have looked at how reflectometry is calculated from a layer model description of the scattering length density profile, and the limitations of the kinematic approach.
Then we have discussed the importance of the differential evolution algorithm in reflectometry analysis and detailed the operation of this algorithm.
Finally, Markov-chain Monte Carlo was introduced in the context of uncertainty quantification for model-dependent analysis.
While not exhaustive, we hope that this document will give you the confidence and understanding to look in more detail into the analysis of reflectometry measurements.

\section*{Author Contributions}
ARM wrote the first draft of this text, with all other authors offering additional content and editing.

\section*{Acknowledgements}
The authors would like to thank Luke Clifton of ISIS Neutron and Muon Source for insightful input on the manuscript.


\bibliography{handout}
\bibliographystyle{unsrtnat}

\end{document}
